import time
import re
import random
from collections import defaultdict, Counter

class SparkSimulatorOneCompiler:
    """Simulador de Spark optimizado para OneDrive"""
    
    def __init__(self, app_name="SparkTextAnalysis"):
        print("ğŸš€ SIMULADOR DE SPARK - ONEDRIVE")
        print("=" * 45)
        self.app_name = app_name
        print(f"âœ… AplicaciÃ³n: {app_name}")
        print("ğŸ“ Simulando conceptos de Apache Spark")
        print()
    
    def generate_small_dataset(self):
        """Genera dataset pequeÃ±o en memoria para ONEDRIVE"""
        print("ğŸ“ Generando dataset de texto...")
        
        words = [
            'spark', 'python', 'data', 'analysis', 'processing', 'machine', 'learning',
            'algorithm', 'model', 'training', 'prediction', 'classification', 'big',
            'dataset', 'framework', 'apache', 'distributed', 'computing', 'optimization',
            'performance', 'scalability', 'the', 'and', 'or', 'but', 'in', 'on', 'at',
            'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through'
        ]
        
        # Generar lÃ­neas en memoria (mÃ¡s eficiente para OneCompiler)
        lines = []
        for _ in range(10000):  # 10K lÃ­neas
            line_length = random.randint(8, 15)
            line_words = [random.choice(words) for _ in range(line_length)]
            line = ' '.join(line_words)
            lines.append(line)
        
        print(f"âœ… Dataset generado: {len(lines)} lÃ­neas en memoria")
        return lines
    
    def clean_word(self, word):
        """Limpia y normaliza palabras"""
        cleaned = re.sub(r'[^\w]', '', word.lower())
        return cleaned if len(cleaned) > 2 else None
    
    def simulate_rdd_operations(self, lines):
        """Simula operaciones RDD: flatMap â†’ map â†’ reduceByKey"""
        print("\nğŸ“ SIMULANDO RDD OPERATIONS")
        print("-" * 30)
        start_time = time.time()
        
        print("ğŸ”„ Paso 1: textFile() - Cargar datos")
        # Simular carga de RDD
        rdd_lines = lines.copy()
        
        print("ğŸ”„ Paso 2: flatMap() - Dividir en palabras")
        # flatMap: cada lÃ­nea â†’ lista de palabras
        all_words = []
        for line in rdd_lines:
            words = line.split()
            all_words.extend(words)
        
        print("ğŸ”„ Paso 3: map() - Limpiar palabras")
        # map: aplicar limpieza a cada palabra
        cleaned_words = []
        for word in all_words:
            cleaned = self.clean_word(word)
            if cleaned:
                cleaned_words.append(cleaned)
        
        print("ğŸ”„ Paso 4: map() - Crear pares (palabra, 1)")
        # map: palabra â†’ (palabra, 1)
        word_pairs = [(word, 1) for word in cleaned_words]
        
        print("ğŸ”„ Paso 5: reduceByKey() - Contar palabras")
        # reduceByKey: agrupar y sumar
        word_counts = defaultdict(int)
        for word, count in word_pairs:
            word_counts[word] += count
        
        print("ğŸ”„ Paso 6: sortBy() - Ordenar resultados")
        # sortBy: ordenar por frecuencia
        sorted_results = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ… RDD completado en {execution_time:.3f} segundos")
        print(f"ğŸ“Š Palabras procesadas: {len(cleaned_words)}")
        print(f"ğŸ“Š Palabras Ãºnicas: {len(word_counts)}")
        
        return execution_time, len(word_counts), sorted_results[:20]
    
    def simulate_dataframe_operations(self, lines):
        """Simula operaciones DataFrame optimizadas"""
        print("\nğŸ“Š SIMULANDO DATAFRAME OPERATIONS")
        print("-" * 35)
        start_time = time.time()
        
        print("ğŸ”„ Paso 1: read.text() - Crear DataFrame")
        # Simular DataFrame
        df_lines = lines.copy()
        
        print("ğŸ”„ Paso 2: explode(split()) - OptimizaciÃ³n vectorizada")
        # OperaciÃ³n vectorizada mÃ¡s eficiente
        all_words = []
        for line in df_lines:
            words = line.split()
            all_words.extend(words)
        
        print("ğŸ”„ Paso 3: Aplicar transformaciones SQL")
        # Simular optimizaciones de Catalyst
        cleaned_words = []
        for word in all_words:
            cleaned = self.clean_word(word)
            if cleaned:
                cleaned_words.append(cleaned)
        
        print("ğŸ”„ Paso 4: groupBy().count() - AgregaciÃ³n optimizada")
        # Counter es mÃ¡s eficiente (simula optimizaciones)
        word_counts = Counter(cleaned_words)
        
        print("ğŸ”„ Paso 5: orderBy() - Ordenamiento optimizado")
        # most_common() es mÃ¡s eficiente
        sorted_results = word_counts.most_common()
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ… DataFrame completado en {execution_time:.3f} segundos")
        print(f"ğŸ“Š Palabras procesadas: {len(cleaned_words)}")
        print(f"ğŸ“Š Palabras Ãºnicas: {len(word_counts)}")
        
        return execution_time, len(word_counts), sorted_results[:20]
    
    def analyze_performance(self, rdd_time, df_time, rdd_results, df_results):
        """Analiza diferencias de rendimiento"""
        print("\n" + "="*50)
        print("ğŸ“ˆ ANÃLISIS DE RENDIMIENTO SPARK")
        print("="*50)
        
        speedup = rdd_time / df_time if df_time > 0 else 1
        efficiency = ((rdd_time - df_time) / rdd_time) * 100 if rdd_time > 0 else 0
        
        print(f"â±ï¸  Tiempo RDD:       {rdd_time:.3f} segundos")
        print(f"â±ï¸  Tiempo DataFrame: {df_time:.3f} segundos")
        print(f"ğŸš€ Speedup:          {speedup:.2f}x")
        print(f"ğŸ“Š Mejora:           {efficiency:.1f}%")
        
        print(f"\nğŸ” COMPARACIÃ“N DETALLADA:")
        print(f"{'MÃ©todo':<12} {'Tiempo':<10} {'Rendimiento'}")
        print("-" * 35)
        print(f"{'RDD':<12} {rdd_time:<10.3f} {'Baseline'}")
        print(f"{'DataFrame':<12} {df_time:<10.3f} {f'{speedup:.1f}x mejor'}")
        
        print(f"\nğŸ’¡ Â¿POR QUÃ‰ DATAFRAMES SON MÃS RÃPIDOS?")
        print("ğŸ”§ Optimizaciones de DataFrames:")
        print("   â€¢ Catalyst Optimizer: Optimiza el plan de ejecuciÃ³n")
        print("   â€¢ Tungsten Engine: GestiÃ³n eficiente de memoria")
        print("   â€¢ Code Generation: Genera cÃ³digo Java optimizado")
        print("   â€¢ Predicate Pushdown: Aplica filtros temprano")
        print("   â€¢ Columnar Storage: Mejor compresiÃ³n y cache")
        
        print("\nâš™ï¸  CaracterÃ­sticas de RDDs:")
        print("   â€¢ API de bajo nivel mÃ¡s flexible")
        print("   â€¢ Control granular de operaciones")
        print("   â€¢ Sin optimizaciones automÃ¡ticas")
        print("   â€¢ Mejor para datos no estructurados")
        
        return speedup, efficiency
    
    def show_top_words(self, results, title="TOP PALABRAS"):
        """Muestra las palabras mÃ¡s frecuentes"""
        print(f"\nğŸ“Š {title}")
        print("-" * 30)
        for i, (word, count) in enumerate(results[:10], 1):
            print(f"{i:2d}. {word:<12} {count:>6}")
    
    def run_complete_analysis(self):
        """Ejecuta anÃ¡lisis completo optimizado para OneCompiler"""
        print("ğŸ¯ INICIANDO ANÃLISIS COMPLETO")
        print("=" * 35)
        
        # 1. Generar dataset en memoria
        lines = self.generate_small_dataset()
        
        # 2. Ejecutar simulaciÃ³n RDD
        rdd_time, rdd_total, rdd_top = self.simulate_rdd_operations(lines)
        
        # 3. Ejecutar simulaciÃ³n DataFrame
        df_time, df_total, df_top = self.simulate_dataframe_operations(lines)
        
        # 4. AnÃ¡lisis de rendimiento
        speedup, efficiency = self.analyze_performance(rdd_time, df_time, rdd_top, df_top)
        
        # 5. Mostrar resultados
        self.show_top_words(rdd_top, "TOP PALABRAS MÃS FRECUENTES")
        
        print(f"\nğŸ‰ ANÃLISIS COMPLETADO")
        print(f"ğŸ“ˆ Speedup obtenido: {speedup:.2f}x")
        print(f"ğŸ“Š Eficiencia ganada: {efficiency:.1f}%")
        print(f"ğŸ”¤ Palabras Ãºnicas encontradas: {rdd_total}")
        
        return {
            'rdd_time': rdd_time,
            'df_time': df_time,
            'speedup': speedup,
            'efficiency': efficiency,
            'total_words': rdd_total
        }

# EJECUCIÃ“N PRINCIPAL OPTIMIZADA PARA ONECOMPILER
if __name__ == "__main__":
    print("ğŸ”¥ APACHE SPARK SIMULATOR")
    print("ğŸŒ Optimizado para OneCompiler")
    print("ğŸ“š Demostrando conceptos de Big Data")
    print()
    
    # Crear y ejecutar simulador
    simulator = SparkSimulatorOneCompiler("TextAnalysisDemo")
    
    try:
        results = simulator.run_complete_analysis()
        
        print(f"\nâœ¨ RESUMEN EJECUTIVO:")
        print(f"ğŸš€ DataFrames son {results['speedup']:.2f}x mÃ¡s rÃ¡pidos")
        print(f"ğŸ“Š Mejora de eficiencia: {results['efficiency']:.1f}%")
        print(f"ğŸ”¤ Total palabras Ãºnicas: {results['total_words']}")
        print(f"\nğŸ“ CONCEPTOS APRENDIDOS:")
        print("   â€¢ RDDs vs DataFrames en Spark")
        print("   â€¢ Optimizaciones del Catalyst Optimizer")
        print("   â€¢ Operaciones de Word Count distribuido")
        print("   â€¢ AnÃ¡lisis de rendimiento en Big Data")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
    
    print(f"\nğŸ¯ Â¡SimulaciÃ³n de Spark completada!")
    print("ğŸ“– CÃ³digo listo para Francisco's Desk")
